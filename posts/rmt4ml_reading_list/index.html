<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Reading group on "Random Matrix Theory and Machine Learning" | Zhenyu Liao's Page</title>
<link rel=stylesheet href=/css/style.css>
<link rel=stylesheet href=/css/fonts.css>
</head>
<body>
<nav>
<ul class=menu>
<li><a href=/>Home</a></li>
<li><a href=/publications/>Publications</a></li>
<li><a href=/projects/>Projects</a></li>
<li><a href=/book/>Book</a></li>
<li><a href=/collaborators/>Collaborators</a></li>
<li><a href=/teaching/>Teaching</a></li>
<li><a href=/links/>Useful Links</a></li>
<li><a href=/index.xml>RSS subscribe</a></li>
</ul>
<hr>
</nav>
<div class=article-meta>
<h1><span class=title>Reading group on &ldquo;Random Matrix Theory and Machine Learning&rdquo;</span></h1>
</div>
<main>
<h3 id=paper-list>Paper List:</h3>
<h4 id=overview-paper>Overview paper</h4>
<ul>
<li>Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. &ldquo;<a href=https://arxiv.org/pdf/2103.09177.pdf>Deep Learning: A Statistical Viewpoint</a>&rdquo;. In: Acta Numerica 30 (May 2021), pp. 87–201</li>
</ul>
<h4 id=on-the-interface-between-rmt-and-deep-neural-networks>On the interface between RMT and deep neural networks:</h4>
<ul>
<li>Lucas Benigni and Sandrine Péché. &ldquo;<a href=https://arxiv.org/pdf/1904.03090.pdf>Eigenvalue Distribution of Some Nonlinear Models of Random Matrices</a>&rdquo;. In: Electronic Journal of Probability 26.none (Jan. 2021), pp. 1–37</li>
<li>Lucas Benigni and Sandrine Péché. &ldquo;<a href=https://arxiv.org/abs/2201.04753>Largest Eigenvalues of the Conjugate Kernel of Single-Layered Neural Networks</a>&rdquo;. Jan. 2022.</li>
<li>Jeffrey Pennington and Pratik Worah. &ldquo;<a href=https://proceedings.neurips.cc/paper_files/paper/2017/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf>Nonlinear random matrix theory for deep learning</a>&rdquo;. In: Advances in Neural Information Processing Systems. 2017, pp. 2634–2643 (GOOD TO KNOW)</li>
<li>Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. &ldquo;<a href=https://arxiv.org/pdf/1711.04735.pdf>Resurrecting the Sigmoid in Deep Learning through Dynamical Isometry: Theory and Practice</a>&rdquo;. In: Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc., 2017, pp. 4785–4795 (GOOD TO KNOW)</li>
<li>Zhou Fan and Zhichao Wang. “Spectra of the Conjugate Kernel and Neural Tangent Kernel for Linear-Width Neural Networks”. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Associates, Inc., 2020, pp. 7710–7721</li>
<li>[P1] Leonid Pastur. “On Random Matrices Arising in Deep Neural Networks. Gaussian Case”. In: arXiv (2020). eprint: 2001.06188</li>
<li>[P2] Leonid Pastur and Victor Slavin. “On Random Matrices Arising in Deep Neural Networks: General I.I.D. Case”. In: Random Matrices: Theory and Applications 12.01 (Jan. 2023), p. 2250046</li>
<li>[P3] Leonid Pastur. “Eigenvalue Distribution of Large Random Matrices Arising in Deep Neural Networks: Orthogonal Case”. In: Journal of Mathematical Physics 63.6 (2022), p. 063505</li>
</ul>
<p>Other connection to high-dimensional asymptotics, etc</p>
<ul>
<li><a href=https://arxiv.org/abs/1902.06720>Wide Neural Networts of Any Depth Evolve As Linear Models Under Gradients Descent</a>, Jaehoon Lee*, Lechao Xiao*, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, Jeffrey Pennington. NeurIPS 2019.</li>
<li>Trevor Hastie et al. “Surprises in High-Dimensional Ridgeless Least Squares Interpolation”. In: The Annals of Statistics 50.2 (Apr. 2022), pp. 949–986</li>
<li>Arthur Jacot, Franck Gabriel, and Clément Hongler. “Neural tangent kernel: Convergence and generalization in neural networks”. In: Advances in neural information processing systems. 2018, pp. 8571–8580</li>
<li>Hu, H., & Lu, Y. M. (2022). Universality laws for high-dimensional learning with random features. <em>IEEE Transactions on Information Theory</em>, <em>69</em>(3), 1932-1964.</li>
</ul>
<p>High-dimensional dynamics of DNN</p>
<ul>
<li>High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation, Jimmy Ba et al. 2022</li>
<li>Moniri, B., Lee, D., Hassani, H., & Dobriban, E. (2023). A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks. <em>arXiv preprint arXiv:2310.07891</em>.</li>
<li>Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. “High-Dimensional Limit Theorems for SGD: Effective Dynamics and Critical Scaling”. In: Advances in Neural Information Processing Systems 35 (Dec. 2022), pp. 25349–25362</li>
<li>Gerard Ben Arous et al. High-Dimensional SGD Aligns with Emerging Outlier Eigenspaces. Oct. 2023. arXiv: 2310.03010</li>
</ul>
<p><strong>More information</strong></p>
<h3 id=contact-and-thanks>Contact and Thanks</h3>
<ul>
<li><a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a>, EIC, Huazhong University of Science and Technology</li>
<li>Prof. <a href="https://math127.nenu.edu.cn/shuxue/WebPage/WebPageView.php?WorkCur=7&WorkType=2000900103">Shurong Zheng</a>, School of Mathematics and Statistics, Northeast Normal University</li>
<li>Prof. <a href=https://sds.cuhk.edu.cn/en/teacher/506>Jeff Yao</a>, School of Data Science, The Chinese University of Hong Kong, Shenzhen</li>
</ul>
<p>The organizers are grateful for support from NSFC via fund NSFC-62206101 and NSFC-12141107.</p>
</main>
<footer>
<script defer src=//zhenyu-liao.github.io/js/math-code.js></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script defer src=//zhenyu-liao.github.io/js/center-img.js></script>
<hr>
© <a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a> 2016 &ndash; 2023 | <a href="https://scholar.google.fr/citations?user=SPYhJV8AAAAJ&hl=en">Google Scholar</a> | <a href=https://www.researchgate.net/profile/Zhenyu_Liao>Research Gate</a> | <a href=https://orcid.org/0000-0002-1915-8559>ORCID</a> | <a href=https://github.com/Zhenyu-LIAO/>Github</a> | <a href=https://zhuanlan.zhihu.com/RandomMatrixTheory>RMT on Zhihu</a>
</footer>
</body>
</html>