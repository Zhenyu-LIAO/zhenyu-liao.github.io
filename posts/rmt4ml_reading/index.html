<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Reading group on RMT and Machine Learning | Zhenyu Liao's Page</title>
<link rel=stylesheet href=/css/style.css>
<link rel=stylesheet href=/css/fonts.css>
</head>
<body>
<nav>
<ul class=menu>
<li><a href=/>Home</a></li>
<li><a href=/publications/>Publications</a></li>
<li><a href=/projects/>Projects</a></li>
<li><a href=/book/>Book</a></li>
<li><a href=/collaborators/>Collaborators</a></li>
<li><a href=/teaching/>Teaching</a></li>
<li><a href=/links/>Useful Links</a></li>
<li><a href=/post_page/>Posts</a></li>
<li><a href=/index.xml>RSS subscribe</a></li>
</ul>
<hr>
</nav>
<div class=article-meta>
<h1><span class=title>Reading group on RMT and Machine Learning</span></h1>
</div>
<main>
<h3 id=schedule>Schedule</h3>
<table>
<thead>
<tr>
<th></th>
<th>Date</th>
<th>Speaker</th>
<th>Papers to be presented</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Dec. 27, 2023</td>
<td>Zhaorui Dong</td>
<td>[B-1-1]</td>
</tr>
<tr>
<td>2</td>
<td>Jan. 3, 2024</td>
<td>Zhuofan Xu</td>
<td>[B-3-5]</td>
</tr>
<tr>
<td>3</td>
<td>Jan. 10, 2024</td>
<td>Xuran Meng</td>
<td>[C-2]</td>
</tr>
<tr>
<td>4</td>
<td>Jan. 17, 2024</td>
<td>Jing Chen</td>
<td>[F-1]</td>
</tr>
<tr>
<td>5</td>
<td>Jan. 24, 2024</td>
<td>Xingkai Wen</td>
<td>[B-3-6]</td>
</tr>
<tr>
<td>6</td>
<td>Jan. 31, 2024</td>
<td>Tingting Zou</td>
<td>[C-5]</td>
</tr>
<tr>
<td>7</td>
<td>Feb. 7, 2024</td>
<td>Mengze Li</td>
<td>[F-3]</td>
</tr>
<tr>
<td>8</td>
<td>Feb. 28, 2024</td>
<td>Xiaoyi Wang</td>
<td>[C-1]</td>
</tr>
<tr>
<td>9</td>
<td>Mar. 6, 2024</td>
<td>Muen Wu</td>
<td>[F-2]</td>
</tr>
<tr>
<td>10</td>
<td>Mar. 13, 2024</td>
<td>Chengmei Niu</td>
<td>[C-3]</td>
</tr>
<tr>
<td>11</td>
<td>Mar. 20, 2024</td>
<td>Zhenyu Liao</td>
<td>[A-1]</td>
</tr>
<tr>
<td>12</td>
<td>Mar. 27, 2024</td>
<td>Zhenyu Liao</td>
<td>[A-1]</td>
</tr>
<tr>
<td>13</td>
<td>Apr. 3, 2024</td>
<td>[empty]</td>
<td>[empty]</td>
</tr>
<tr>
<td>14</td>
<td>Apr. 10, 2024</td>
<td>Zhenyu Liao</td>
<td>[A-1]</td>
</tr>
<tr>
<td>15</td>
<td>Apr. 17, 2024</td>
<td>Mengze Li</td>
<td>[B-3-2]</td>
</tr>
<tr>
<td>16</td>
<td>Apr. 24, 2024</td>
<td>Xuran Meng</td>
<td>[F-6]</td>
</tr>
<tr>
<td>17</td>
<td>May 1, 2024</td>
<td>[empty]</td>
<td>[empty]</td>
</tr>
<tr>
<td>18</td>
<td>May 8, 2024</td>
<td>Tingting Zou</td>
<td>[H-1]</td>
</tr>
<tr>
<td>19</td>
<td>May 15, 2024</td>
<td>Yongqi Du</td>
<td>[D-1]</td>
</tr>
</tbody>
</table>
<h3 id=list-of-papers>List of papers</h3>
<h4 id=a-overview-paper>[A] Overview paper</h4>
<ul>
<li><input checked disabled type=checkbox> [A-1] Peter L. Bartlett, Andrea Montanari, and Alexander Rakhlin. &ldquo;<a href=https://arxiv.org/pdf/2103.09177.pdf>Deep Learning: A Statistical Viewpoint</a>&rdquo;. In: Acta Numerica 30 (May 2021), pp. 87–201</li>
</ul>
<h4 id=b-on-the-interface-between-rmt-and-deep-neural-networks>[B] On the interface between RMT and deep neural networks:</h4>
<h5 id=b-1-pasturs-papers>[B-1] Pastur&rsquo;s papers</h5>
<ul>
<li><input checked disabled type=checkbox> [B-1-1] Leonid Pastur. &ldquo;<a href=https://arxiv.org/pdf/2001.06188.pdf>On Random Matrices Arising in Deep Neural Networks. Gaussian Case</a>&rdquo;. 2020.</li>
<li>[][B-1-2] Leonid Pastur and Victor Slavin. &ldquo;<a href=https://arxiv.org/pdf/2011.11439.pdf>On Random Matrices Arising in Deep Neural Networks: General I.I.D. Case</a>&rdquo;. In: Random Matrices: Theory and Applications 12.01 (Jan. 2023), p. 2250046</li>
<li>[][B-1-3] Leonid Pastur. &ldquo;<a href=https://arxiv.org/pdf/2201.04543.pdf>Eigenvalue Distribution of Large Random Matrices Arising in Deep Neural Networks: Orthogonal Case</a>&rdquo;. In: Journal of Mathematical Physics 63.6 (2022), p. 063505</li>
</ul>
<h5 id=b-2-péchés-papers>[B-2] Péché&rsquo;s papers</h5>
<ul>
<li>[][B-2-1] Lucas Benigni and Sandrine Péché. &ldquo;<a href=https://arxiv.org/pdf/1904.03090.pdf>Eigenvalue Distribution of Some Nonlinear Models of Random Matrices</a>&rdquo;. In: Electronic Journal of Probability 26.none (Jan. 2021), pp. 1–37</li>
<li>[][B-2-2] Lucas Benigni and Sandrine Péché. &ldquo;<a href=https://arxiv.org/abs/2201.04753>Largest Eigenvalues of the Conjugate Kernel of Single-Layered Neural Networks</a>&rdquo;. 2022.</li>
</ul>
<h5 id=b-3-others>[B-3] Others</h5>
<ul>
<li>[][B-3-1] Jeffrey Pennington and Pratik Worah. &ldquo;<a href=https://proceedings.neurips.cc/paper_files/paper/2017/file/0f3d014eead934bbdbacb62a01dc4831-Paper.pdf>Nonlinear random matrix theory for deep learning</a>&rdquo;. In: Advances in Neural Information Processing Systems. 2017, pp. 2634–2643 (GOOD TO KNOW)</li>
<li><input checked disabled type=checkbox> [B-3-2] Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. &ldquo;<a href=https://arxiv.org/pdf/1711.04735.pdf>Resurrecting the Sigmoid in Deep Learning through Dynamical Isometry: Theory and Practice</a>&rdquo;. In: Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc., 2017, pp. 4785–4795 (GOOD TO KNOW)</li>
<li>[][B-3-3] Charles H. Martin, Michael W. Mahoney. &ldquo;<a href=https://web3.arxiv.org/pdf/1810.01075.pdf>Implicit Self-Regularization in Deep Neural Networks: Evidence from Random Matrix Theory and Implications for Learning</a>&rdquo;. 2018. (mostly empirical)</li>
<li>[][B-3-4] Charles H. Martin, Michael W. Mahoney. &ldquo;<a href=https://arxiv.org/pdf/1901.08276.pdf>Traditional and Heavy-Tailed Self Regularization in Neural Network Models</a>&rdquo;. 2019. (mostly empirical)</li>
<li><input checked disabled type=checkbox> [B-3-5] Yaoqing Yang, Ryan Theisen, Liam Hodgkinson, Joseph E. Gonzalez, Kannan Ramchandran, Charles H. Martin, Michael W. Mahoney. &ldquo;<a href=https://arxiv.org/pdf/2202.02842.pdf>Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data</a>&rdquo;. 2022. (mostly empirical)</li>
<li><input checked disabled type=checkbox> [B-3-6] Matthias Thamm, Max Staats, Bernd Rosenow. &ldquo;<a href=https://arxiv.org/pdf/2203.14661.pdf>Random matrix analysis of deep neural network weight matrices</a>&rdquo;. PhysRevE. 2022.</li>
</ul>
<h4 id=c-double-descent>[C] Double descent</h4>
<ul>
<li><input checked disabled type=checkbox> [C-1] Trevor Hastie, Andrea Montanari, Saharon Rosset, Ryan J. Tibshirani. &ldquo;<a href=https://arxiv.org/pdf/1903.08560.pdf>Surprises in High-Dimensional Ridgeless Least Squares Interpolation</a>&rdquo;. In: The Annals of Statistics 50.2 (Apr. 2022), pp. 949–986</li>
<li><input checked disabled type=checkbox> [C-2] Song Mei and Andrea Montanari. &ldquo;<a href=https://arxiv.org/pdf/1908.05355.pdf>The generalization error of random features regression: Precise asymptotics and double descent curve</a>&rdquo;. Communications on Pure and Applied Mathematics, 2021.</li>
<li><input checked disabled type=checkbox> [C-3] Denny Wu and Ji Xu. &ldquo;<a href=https://arxiv.org/pdf/2006.05800.pdf>On the Optimal Weighted ell 2 Regularization in
Overparameterized Linear Regression</a>&rdquo;. NeurIPS 2020.</li>
<li>[][C-4] Ben Adlam, Jeffrey Pennington. &ldquo;<a href=https://arxiv.org/pdf/2008.06786.pdf>The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization</a>&rdquo;. ICML 2020.</li>
<li><input checked disabled type=checkbox> [C-5] Francis Bach. &ldquo;<a href=https://arxiv.org/pdf/2303.01372.pdf>High-dimensional analysis of double descent for linear regression with random projections</a>&rdquo;. 2023.</li>
</ul>
<h4 id=d-neural-tangent-kernel-and-linearized-neural-networks>[D] Neural Tangent Kernel and linearized neural networks</h4>
<ul>
<li><input checked disabled type=checkbox> [D-1] Arthur Jacot, Franck Gabriel, and Clément Hongler. &ldquo;<a href=https://arxiv.org/pdf/1806.07572.pdf>Neural tangent kernel: Convergence and generalization in neural networks</a>&rdquo;. In: Advances in neural information processing systems. 2018, pp. 8571–8580</li>
<li>[][D-2] Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, Jeffrey Pennington. &ldquo;<a href=https://arxiv.org/abs/1902.06720>Wide Neural Networts of Any Depth Evolve As Linear Models Under Gradients Descent</a>&rdquo;. NeurIPS 2019.</li>
<li>[][D-3] Ph.D. thesis of Arthur Jacor, &ldquo;<a href="https://infoscience.epfl.ch/record/295831?ln=en">Theory of Deep Learning: Neural Tangent Kernel and Beyond</a>&rdquo;, 2023.</li>
<li>[][D-4] Zhou Fan and Zhichao Wang. &ldquo;<a href=https://arxiv.org/pdf/2005.11879.pdf>Spectra of the Conjugate Kernel and Neural Tangent Kernel for Linear-Width Neural Networks</a>&rdquo;. In: Advances in Neural Information Processing Systems. Vol. 33. Curran Associates, Inc., 2020, pp. 7710–7721</li>
<li>[][D-5] Hong Hu, Yue M. Lu. &ldquo;<a href=https://arxiv.org/pdf/2009.07669.pdf>Universality laws for high-dimensional learning with random features</a>. IEEE Transactions on Information Theory 69 (3), 1932-1964. 2022.</li>
<li>[][D-6] Zhichao Wang, Andrew Engel, Anand Sarwate, Ioana Dumitriu, Tony Chiang. &ldquo;<a href=https://arxiv.org/pdf/2211.06506.pdf>Spectral evolution and invariance in linear-width neural networks</a>&rdquo;. 2022.</li>
<li>[][D-7] Theodor Misiakiewicz and Andrea Montanari, &ldquo;<a href=https://web.stanford.edu/~montanar/RESEARCH/FILEPAP/linear-nets.pdf>Six Lectures on Linearized Neural Networks</a>&rdquo;. 2023</li>
</ul>
<h4 id=e-high-dimensional-dynamics-of-dnn>[E] High-dimensional dynamics of DNN</h4>
<ul>
<li>[][E-1] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, Greg Yang. &ldquo;<a href=https://arxiv.org/pdf/2205.01445.pdf>High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation</a>&rdquo;. 2022</li>
<li>[][E-2] Behrad Moniri, Donghwan Lee, Hamed Hassani, Edgar Dobriban. &ldquo;<a href=https://arxiv.org/pdf/2310.07891.pdf>A Theory of Non-Linear Feature Learning with One Gradient Step in Two-Layer Neural Networks</a>&rdquo;. 2023</li>
<li>[][E-3] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. &ldquo;<a href=https://arxiv.org/pdf/2206.04030.pdf>High-Dimensional Limit Theorems for SGD: Effective Dynamics and Critical Scaling</a>&rdquo;. In: Advances in Neural Information Processing Systems 35 (Dec. 2022), pp. 25349–25362</li>
<li>[][E-4] Gerard Ben Arous, Reza Gheissari, Jiaoyang Huang, Aukosh Jagannath. &ldquo;<a href=https://arxiv.org/pdf/2310.03010.pdf>High-Dimensional SGD Aligns with Emerging Outlier Eigenspaces</a>&rdquo;. 2023.</li>
</ul>
<h4 id=f-transformer-based-model-and-in-context-learning>[F] Transformer-based model and in-context learning</h4>
<ul>
<li><input checked disabled type=checkbox> [F-1] Shivam Garg, Dimitris Tsipras, Percy Liang, Gregory Valiant. &ldquo;<a href=https://arxiv.org/pdf/2208.01066.pdf>What Can Transformers Learn In-Context? A Case Study of Simple Function Classes</a>&rdquo;. NeurISP 2022.</li>
<li><input checked disabled type=checkbox> [F-2] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou. &ldquo;<a href=https://arxiv.org/pdf/2211.15661.pdf>What learning algorithm is in-context learning? Investigations with linear models</a>&rdquo;. ICLR 2023.</li>
<li><input checked disabled type=checkbox> [F-3] Johannes von Oswald et al. &ldquo;<a href=https://arxiv.org/pdf/2212.07677.pdf>Transformers Learn In-Context by Gradient Descent</a>&rdquo;. ICML 2022.</li>
<li>[][F-4] Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, Samet Oymak. &ldquo;<a href=https://proceedings.mlr.press/v202/li23l/li23l.pdf>Transformers as Algorithms: Generalization and Stability in In-context Learning</a>&rdquo;. ICML 2023.</li>
<li>[][F-5] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei. &ldquo;<a href=https://arxiv.org/pdf/2306.04637.pdf>Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection</a>&rdquo;. 2023.</li>
<li><input checked disabled type=checkbox> [F-6] Zhang, Ruiqi, Spencer Frei, and Peter L. Bartlett. &ldquo;<a href=https://arxiv.org/pdf/2306.09927.pdf>Trained Transformers Learn Linear Models In-Context</a>&rdquo;. 2023. arXiv preprint arXiv:2306.09927 (2023).</li>
</ul>
<h4 id=h-random-kernel-matrices>[H] Random kernel matrices</h4>
<ul>
<li><input checked disabled type=checkbox> [H-1] Xiuyuan Cheng, Amit Singer. &ldquo;<a href=https://arxiv.org/pdf/1202.3155.pdf>The Spectrum of Random Inner-product Kernel Matrices</a>&rdquo;. 2012.</li>
<li>[][H-2] Zhou Fan, Andrea Montanari. &ldquo;<a href=https://arxiv.org/pdf/1507.05343.pdf>The Spectral Norm of Random Inner-Product Kernel Matrices</a>&rdquo;. 2017.</li>
<li>[][H-3] Yue M. Lu, Horng-Tzer Yau. &ldquo;<a href=https://arxiv.org/pdf/2205.06308.pdf>An Equivalence Principle for the Spectrum of Random Inner-Product Kernel Matrices with Polynomial Scalings</a>&rdquo;. 2023.</li>
</ul>
<h3 id=more-information>More information</h3>
<h3 id=contact-and-thanks>Contact and Thanks</h3>
<ul>
<li><a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a>, EIC, Huazhong University of Science and Technology</li>
<li>Prof. <a href="https://math127.nenu.edu.cn/shuxue/WebPage/WebPageView.php?WorkCur=7&WorkType=2000900103">Shurong Zheng</a>, School of Mathematics and Statistics, Northeast Normal University</li>
<li>Prof. <a href=https://sds.cuhk.edu.cn/en/teacher/506>Jeff Yao</a>, School of Data Science, The Chinese University of Hong Kong, Shenzhen</li>
</ul>
<p>The organizers are grateful for support from NSFC via fund NSFC-62206101 and NSFC-12141107.</p>
</main>
<footer>
<script defer src=//zhenyu-liao.github.io/js/math-code.js></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script defer src=//zhenyu-liao.github.io/js/center-img.js></script>
<hr>
© <a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a> 2016 &ndash; 2023 | <a href="https://scholar.google.fr/citations?user=SPYhJV8AAAAJ&hl=en">Google Scholar</a> | <a href=https://www.researchgate.net/profile/Zhenyu_Liao>Research Gate</a> | <a href=https://orcid.org/0000-0002-1915-8559>ORCID</a> | <a href=https://github.com/Zhenyu-LIAO/>Github</a> | <a href=https://zhuanlan.zhihu.com/RandomMatrixTheory>RMT on Zhihu</a>
</footer>
</body>
</html>