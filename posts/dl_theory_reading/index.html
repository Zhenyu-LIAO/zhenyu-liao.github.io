<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Reading group on Modern DL Theory and Practice | Zhenyu Liao's Page</title>
<link rel=stylesheet href=/css/style.css>
<link rel=stylesheet href=/css/fonts.css>
</head>
<body>
<nav>
<ul class=menu>
<li><a href=/>Home</a></li>
<li><a href=/publications/>Publications</a></li>
<li><a href=/projects/>Projects</a></li>
<li><a href=/book/>Book</a></li>
<li><a href=/collaborators/>Collaborators</a></li>
<li><a href=/teaching/>Teaching</a></li>
<li><a href=/links/>Useful Links</a></li>
<li><a href=/post_page/>Posts</a></li>
<li><a href=/index.xml>RSS subscribe</a></li>
</ul>
<hr>
</nav>
<div class=article-meta>
<h1><span class=title>Reading group on Modern DL Theory and Practice</span></h1>
</div>
<main>
<h3 id=schedule>Schedule</h3>
<h3 id=list-of-papers>List of papers</h3>
<h4 id=a-tensor-program>[A] Tensor Program</h4>
<ul>
<li>
<p>[][A-1] Yang, Greg. &ldquo;<a href=https://proceedings.neurips.cc/paper_files/paper/2019/file/5e69fda38cda2060819766569fd93aa5-Paper.pdf>Wide feedforward or recurrent neural networks of any architecture are gaussian processes</a>.&rdquo; Advances in Neural Information Processing Systems 32 (2019).</p>
</li>
<li>
<p>[][A-2] Yang, Greg. &ldquo;<a href=https://arxiv.org/pdf/2006.14548.pdf>Tensor programs ii: Neural tangent kernel for any architecture</a>.&rdquo; arXiv preprint arXiv:2006.14548 (2020).</p>
</li>
<li>
<p>[][A-3] Yang, Greg, and Etai Littwin. &ldquo;<a href=https://proceedings.mlr.press/v139/yang21f/yang21f.pdf>Tensor programs iib: Architectural universality of neural tangent kernel training dynamics</a>.&rdquo; International Conference on Machine Learning. PMLR, 2021.</p>
</li>
<li>
<p>[][A-4] Yang, Greg, et al. &ldquo;<a href=https://arxiv.org/pdf/2310.02244.pdf>Tensor programs vi: Feature learning in infinite-depth neural networks</a>.&rdquo; arXiv preprint arXiv:2310.02244 (2023).</p>
</li>
<li>
<p>[][A-5] Noci, Lorenzo, et al. &ldquo;<a href=https://arxiv.org/pdf/2402.17457v1.pdf>Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning</a>.&rdquo; arXiv preprint arXiv:2402.17457 (2024).</p>
</li>
</ul>
<h3 id=more-information>More information</h3>
<h3 id=contact-and-thanks>Contact and Thanks</h3>
<ul>
<li><a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a>, EIC, Huazhong University of Science and Technology</li>
</ul>
<p>The organizers are grateful for support from NSFC via fund NSFC-62206101.</p>
</main>
<footer>
<script defer src=//zhenyu-liao.github.io/js/math-code.js></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script defer src=//zhenyu-liao.github.io/js/center-img.js></script>
<hr>
Â© <a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a> 2016 &ndash; 2023 | <a href="https://scholar.google.fr/citations?user=SPYhJV8AAAAJ&hl=en">Google Scholar</a> | <a href=https://www.researchgate.net/profile/Zhenyu_Liao>Research Gate</a> | <a href=https://orcid.org/0000-0002-1915-8559>ORCID</a> | <a href=https://github.com/Zhenyu-LIAO/>Github</a> | <a href=https://zhuanlan.zhihu.com/RandomMatrixTheory>RMT on Zhihu</a>
</footer>
</body>
</html>