<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Reading group on Modern Deep Learning Theory and Practice | Zhenyu Liao's Page</title>
<link rel=stylesheet href=/css/style.css>
<link rel=stylesheet href=/css/fonts.css>
</head>
<body>
<nav>
<ul class=menu>
<li><a href=/>Home</a></li>
<li><a href=/publications/>Publications</a></li>
<li><a href=/projects/>Projects</a></li>
<li><a href=/book/>Book</a></li>
<li><a href=/teaching/>Teaching</a></li>
<li><a href=/links/>Links</a></li>
<li><a href=/post_page/>Posts</a></li>
<li><a href=/index.xml>RSS subscribe</a></li>
</ul>
<hr>
</nav>
<div class=article-meta>
<h1><span class=title>Reading group on Modern Deep Learning Theory and Practice</span></h1>
</div>
<main>
<h3 id=schedule>Schedule</h3>
<table>
<thead>
<tr>
<th></th>
<th>Date</th>
<th>Speaker</th>
<th>Papers to be presented</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>June 14, 2024</td>
<td>Yanlei Liu</td>
<td>[D-9]</td>
</tr>
<tr>
<td>2</td>
<td>June 21, 2024</td>
<td>Chengmei Niu</td>
<td>[D-8]</td>
</tr>
<tr>
<td>3</td>
<td>June 28, 2024</td>
<td>Jaiqing Liu</td>
<td>[D-2]</td>
</tr>
<tr>
<td>4</td>
<td>July 05, 2024</td>
<td>Kexin Chen</td>
<td>[D-10]</td>
</tr>
<tr>
<td>5</td>
<td>July 12, 2024</td>
<td>Muen Wu</td>
<td>[D-5]</td>
</tr>
<tr>
<td>6</td>
<td>July 19, 2024</td>
<td>Yue Xu</td>
<td>[D-4]</td>
</tr>
</tbody>
</table>
<h3 id=list-of-papers>List of papers</h3>
<h4 id=a-tensor-program>[A] Tensor Program</h4>
<ul>
<li>
<p>[][A-1] Yang, Greg. &ldquo;<a href=https://proceedings.neurips.cc/paper_files/paper/2019/file/5e69fda38cda2060819766569fd93aa5-Paper.pdf>Wide feedforward or recurrent neural networks of any architecture are gaussian processes</a>.&rdquo; Advances in Neural Information Processing Systems 32 (2019).</p>
</li>
<li>
<p>[][A-2] Yang, Greg. &ldquo;<a href=https://arxiv.org/pdf/2006.14548.pdf>Tensor programs ii: Neural tangent kernel for any architecture</a>.&rdquo; arXiv preprint arXiv:2006.14548 (2020).</p>
</li>
<li>
<p>[][A-3] Yang, Greg, and Etai Littwin. &ldquo;<a href=https://proceedings.mlr.press/v139/yang21f/yang21f.pdf>Tensor programs iib: Architectural universality of neural tangent kernel training dynamics</a>.&rdquo; International Conference on Machine Learning. PMLR, 2021.</p>
</li>
<li>
<p>[][A-4] Yang, Greg, et al. &ldquo;<a href=https://arxiv.org/pdf/2310.02244.pdf>Tensor programs vi: Feature learning in infinite-depth neural networks</a>.&rdquo; arXiv preprint arXiv:2310.02244 (2023).</p>
</li>
<li>
<p>[][A-5] Noci, Lorenzo, et al. &ldquo;<a href=https://arxiv.org/pdf/2402.17457v1.pdf>Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning</a>.&rdquo; arXiv preprint arXiv:2402.17457 (2024).</p>
</li>
<li>
<p>[][A-6] Li, Ping, and Phan-Minh Nguyen. &ldquo;<a href="https://openreview.net/pdf?id=HJx54i05tX">On random deep weight-tied autoencoders: Exact asymptotic analysis, phase transitions, and implications to training</a>.&rdquo; International Conference on Learning Representations. 2018.</p>
</li>
<li>
<p>[][A-7] Noci, Lorenzo, et al. &ldquo;<a href=https://arxiv.org/pdf/2402.17457v1.pdf>Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning</a>.&rdquo; arXiv preprint arXiv:2402.17457 (2024).</p>
</li>
</ul>
<h4 id=b-theory-and-practice-for-transformers>[B] Theory and Practice for Transformers</h4>
<ul>
<li>
<p>[][B-1] Cowsik, Aditya, et al. &ldquo;<a href=https://arxiv.org/pdf/2403.02579v1.pdf>Geometric Dynamics of Signal Propagation Predict Trainability of Transformers</a>.&rdquo; arXiv preprint arXiv:2403.02579 (2024).</p>
</li>
<li>
<p>[][B-2] Noci, Lorenzo, et al. &ldquo;<a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/ae0cba715b60c4052359b3d52a2cff7f-Paper-Conference.pdf>Signal propagation in transformers: Theoretical perspectives and the role of rank collapse</a>.&rdquo; Advances in Neural Information Processing Systems 35 (2022): 27198-27211.</p>
</li>
<li>
<p>[][B-3] Malladi, Sadhika, et al. &ldquo;<a href=https://proceedings.mlr.press/v202/malladi23a/malladi23a.pdf>A kernel-based view of language model fine-tuning</a>.&rdquo; International Conference on Machine Learning. PMLR, 2023.</p>
</li>
<li>
<p>[][B-4] Hayou, Soufiane, Nikhil Ghosh, and Bin Yu. &ldquo;<a href=https://arxiv.org/pdf/2402.12354.pdf>LoRA+: Efficient Low Rank Adaptation of Large Models</a>.&rdquo; arXiv preprint arXiv:2402.12354 (2024). Together with the original <a href=https://arxiv.org/pdf/2106.09685>LoRA paper</a></p>
</li>
</ul>
<h4 id=c-random-kernel-matrices>[C] Random kernel matrices</h4>
<ul>
<li><input checked disabled type=checkbox> [C-1] Xiuyuan Cheng, Amit Singer. &ldquo;<a href=https://arxiv.org/pdf/1202.3155.pdf>The Spectrum of Random Inner-product Kernel Matrices</a>&rdquo;. 2012.</li>
<li><input checked disabled type=checkbox> [C-2] Zhou Fan, Andrea Montanari. &ldquo;<a href=https://arxiv.org/pdf/1507.05343.pdf>The Spectral Norm of Random Inner-Product Kernel Matrices</a>&rdquo;. 2017.</li>
<li><input checked disabled type=checkbox> [C-3] Z. Liao, R. Couillet, &ldquo;<a href=https://arxiv.org/abs/1909.06788>Inner-product Kernels are Asymptotically Equivalent to Binary Discrete Kernels</a>&rdquo;, 2019.</li>
<li><input checked disabled type=checkbox> [C-4] Z. Liao, R. Couillet, and M. Mahoney. &ldquo;<a href="https://openreview.net/forum?id=pBqLS-7KYAF">Sparse Quantized Spectral Clustering</a>.&rdquo; 2021.</li>
<li><input checked disabled type=checkbox> [C-5] Yue M. Lu, Horng-Tzer Yau. &ldquo;<a href=https://arxiv.org/pdf/2205.06308.pdf>An Equivalence Principle for the Spectrum of Random Inner-Product Kernel Matrices with Polynomial Scalings</a>&rdquo;. 2023.</li>
<li><input checked disabled type=checkbox> [C-6] Sofiia Dubova et al. &ldquo;<a href=https://arxiv.org/pdf/2310.18280.pdf>Universality for the global spectrum of random inner-product kernel matrices in the polynomial regime</a>.&rdquo; 2023.</li>
</ul>
<h4 id=d-transformer-based-model-and-in-context-learning>[D] Transformer-based model and in-context learning</h4>
<ul>
<li>[][D-1] Ruiqi Zhang, Spencer Frei, and Peter L. Bartlett. &ldquo;<a href=https://jmlr.org/papers/volume25/23-1042/23-1042.pdf>Trained transformers learn linear models in-context</a>&rdquo;. Journal of Machine Learning Research, 25(49):1–55, 2024.</li>
<li><input checked disabled type=checkbox> [D-2] Shivam Garg, Dimitris Tsipras, Percy Liang, Gregory Valiant. &ldquo;<a href=https://arxiv.org/pdf/2208.01066.pdf>What Can Transformers Learn In-Context? A Case Study of Simple Function Classes</a>&rdquo;. NeurISP 2022.</li>
<li>[][D-3] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, Denny Zhou. &ldquo;<a href=https://arxiv.org/pdf/2211.15661.pdf>What learning algorithm is in-context learning? Investigations with linear models</a>&rdquo;. ICLR 2023.</li>
<li><input checked disabled type=checkbox> [D-4] Johannes von Oswald et al. &ldquo;<a href=https://arxiv.org/pdf/2212.07677.pdf>Transformers Learn In-Context by Gradient Descent</a>&rdquo;. ICML 2022.</li>
<li><input checked disabled type=checkbox> [D-5] Yingcong Li, M. Emrullah Ildiz, Dimitris Papailiopoulos, Samet Oymak. &ldquo;<a href=https://proceedings.mlr.press/v202/li23l/li23l.pdf>Transformers as Algorithms: Generalization and Stability in In-context Learning</a>&rdquo;. ICML 2023.</li>
<li>[][D-6] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei. &ldquo;<a href=https://arxiv.org/pdf/2306.04637.pdf>Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection</a>&rdquo;. 2023.</li>
<li>[][D-7] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter Bartlett. &ldquo;<a href="https://openreview.net/pdf?id=vSh5ePa0ph">How many pretraining tasks are needed for in-context learning of linear regression?</a>&rdquo;. ICLR 2024.</li>
<li><input checked disabled type=checkbox> [D-8] Yue M. Lu, Mary I. Leteya, Jacob A. Zavatone-Veth, Anindita Maiti, and Cengiz Pehlevan. &ldquo;<a href=https://arxiv.org/pdf/2405.11751>Asymptotic theory of in-context learning by linear attention</a>&rdquo;. 2024.</li>
<li><input checked disabled type=checkbox> [D-9] Aaditya K Singh, Stephanie C.Y. Chan, Ted Moskovitz, Erin Grant, Andrew M Saxe, Felix Hill. &ldquo;<a href="https://openreview.net/forum?id=Of0GBzow8P">The Transient Nature of Emergent In-Context Learning in Transformers</a>&rdquo;. NeurIPS 2023.</li>
<li><input checked disabled type=checkbox> [D-10] Allan Raventos, Mansheej Paul, Feng Chen, Surya Ganguli. &ldquo;<a href="https://openreview.net/pdf?id=BtAz4a5xDg">Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression</a>&rdquo;. NeurIPS 2023.</li>
<li>[][D-11] Siyu Chen, Heejune Sheen, Tianhao Wang, Zhuoran Yang. &ldquo;<a href=https://arxiv.org/pdf/2402.19442>Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality</a>&rdquo;. 2024.</li>
</ul>
<h4 id=f-others>[F] Others</h4>
<ul>
<li>
<p>[][F-1] Bordelon, Blake, Alexander Atanasov, and Cengiz Pehlevan. &ldquo;<a href=https://arxiv.org/pdf/2402.01092v1.pdf>A Dynamical Model of Neural Scaling Laws</a>.&rdquo; arXiv preprint arXiv:2402.01092 (2024).</p>
</li>
<li>
<p>[][F-2] Bahri, Yasaman et al. &ldquo;<a href=https://arxiv.org/pdf/2102.06701.pdf>Explaining Neural Scaling Laws</a>.&rdquo; 2021.</p>
</li>
<li>
<p>[][F-3] Kumar, Tanishq, et al. &ldquo;<a href=https://arxiv.org/pdf/2310.06110.pdf>Grokking as the transition from lazy to rich training dynamics</a>.&rdquo; arXiv preprint arXiv:2310.06110 (2023).</p>
</li>
<li>
<p>[][F-4] Papyan, Vardan, X. Y. Han, and David L. Donoho. &ldquo;<a href=https://www.pnas.org/doi/full/10.1073/pnas.2015509117>Prevalence of neural collapse during the terminal phase of deep learning training</a>.&rdquo; Proceedings of the National Academy of Sciences 117.40 (2020): 24652-24663.</p>
</li>
<li>
<p>[][F-5] Adityanarayanan Radhakrishnan et al. &ldquo;<a href=https://arxiv.org/pdf/2212.13881.pdf>Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features</a>.&rdquo; 2023.</p>
</li>
<li>
<p>[][F-6] Noam Levi, Alon Beck, and Yohai Bar Sinai. &ldquo;<a href=https://arxiv.org/pdf/2310.16441.pdf>Grokking in Linear Estimators – A Solvable Model that Groks without Understanding</a>.&rdquo; 2023.</p>
</li>
</ul>
<h3 id=more-information>More information</h3>
<h3 id=contact-and-thanks>Contact and Thanks</h3>
<ul>
<li><a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a>, EIC, Huazhong University of Science and Technology</li>
</ul>
<p>The organizers are grateful for support from NSFC via fund NSFC-62206101.</p>
</main>
<footer>
<script defer src=//zhenyu-liao.github.io/js/math-code.js></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script defer src=//zhenyu-liao.github.io/js/center-img.js></script>
<hr>
© <a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a> 2016 &ndash; 2026 | <a href="https://scholar.google.fr/citations?user=SPYhJV8AAAAJ&hl=en">Google Scholar</a> | <a href=https://www.researchgate.net/profile/Zhenyu_Liao>Research Gate</a> | <a href=https://orcid.org/0000-0002-1915-8559>ORCID</a> | <a href=https://github.com/Zhenyu-LIAO/>Github</a> | <a href=https://rmta-seminar.github.io/>RMTA Seminar</a>
</footer>
</body>
</html>