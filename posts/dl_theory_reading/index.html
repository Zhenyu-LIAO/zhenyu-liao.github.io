<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Reading group on Modern Deep Learning Theory and Practice | Zhenyu Liao's Page</title>
<link rel=stylesheet href=/css/style.css>
<link rel=stylesheet href=/css/fonts.css>
</head>
<body>
<nav>
<ul class=menu>
<li><a href=/>Home</a></li>
<li><a href=/publications/>Publications</a></li>
<li><a href=/projects/>Projects</a></li>
<li><a href=/book/>Book</a></li>
<li><a href=/collaborators/>Collaborators</a></li>
<li><a href=/teaching/>Teaching</a></li>
<li><a href=/links/>Useful Links</a></li>
<li><a href=/post_page/>Posts</a></li>
<li><a href=/index.xml>RSS subscribe</a></li>
</ul>
<hr>
</nav>
<div class=article-meta>
<h1><span class=title>Reading group on Modern Deep Learning Theory and Practice</span></h1>
</div>
<main>
<h3 id=schedule>Schedule</h3>
<table>
<thead>
<tr>
<th></th>
<th>Date</th>
<th>Speaker</th>
<th>Papers to be presented</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Mar. 05, 2024</td>
<td>Tianqi Hou</td>
<td>Overview of [A]</td>
</tr>
<tr>
<td>2</td>
<td>Mar. 12, 2024</td>
<td>Jiaqing Liu</td>
<td>[A-1]</td>
</tr>
<tr>
<td>3</td>
<td>Mar. 19, 2024</td>
<td>Yue Xu</td>
<td>[A-5]</td>
</tr>
<tr>
<td>4</td>
<td>Mar. 26, 2024</td>
<td>Muen Wu</td>
<td>[A-2]</td>
</tr>
<tr>
<td>5</td>
<td>Apr. 02, 2024</td>
<td>Chengmei Niu</td>
<td>[A-3]</td>
</tr>
<tr>
<td>6</td>
<td>Apr. 09, 2024</td>
<td>Bin Li</td>
<td>[A-4]</td>
</tr>
<tr>
<td>7</td>
<td>Apr. 16, 2024</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h3 id=list-of-papers>List of papers</h3>
<h4 id=a-tensor-program>[A] Tensor Program</h4>
<ul>
<li>
<p>[][A-1] Yang, Greg. &ldquo;<a href=https://proceedings.neurips.cc/paper_files/paper/2019/file/5e69fda38cda2060819766569fd93aa5-Paper.pdf>Wide feedforward or recurrent neural networks of any architecture are gaussian processes</a>.&rdquo; Advances in Neural Information Processing Systems 32 (2019).</p>
</li>
<li>
<p>[][A-2] Yang, Greg. &ldquo;<a href=https://arxiv.org/pdf/2006.14548.pdf>Tensor programs ii: Neural tangent kernel for any architecture</a>.&rdquo; arXiv preprint arXiv:2006.14548 (2020).</p>
</li>
<li>
<p>[][A-3] Yang, Greg, and Etai Littwin. &ldquo;<a href=https://proceedings.mlr.press/v139/yang21f/yang21f.pdf>Tensor programs iib: Architectural universality of neural tangent kernel training dynamics</a>.&rdquo; International Conference on Machine Learning. PMLR, 2021.</p>
</li>
<li>
<p>[][A-4] Yang, Greg, et al. &ldquo;<a href=https://arxiv.org/pdf/2310.02244.pdf>Tensor programs vi: Feature learning in infinite-depth neural networks</a>.&rdquo; arXiv preprint arXiv:2310.02244 (2023).</p>
</li>
<li>
<p>[][A-5] Noci, Lorenzo, et al. &ldquo;<a href=https://arxiv.org/pdf/2402.17457v1.pdf>Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning</a>.&rdquo; arXiv preprint arXiv:2402.17457 (2024).</p>
</li>
<li>
<p>[][A-6] Li, Ping, and Phan-Minh Nguyen. &ldquo;<a href="https://openreview.net/pdf?id=HJx54i05tX">On random deep weight-tied autoencoders: Exact asymptotic analysis, phase transitions, and implications to training</a>.&rdquo; International Conference on Learning Representations. 2018.</p>
</li>
<li>
<p>[][A-7] Noci, Lorenzo, et al. &ldquo;<a href=https://arxiv.org/pdf/2402.17457v1.pdf>Why do Learning Rates Transfer? Reconciling Optimization and Scaling Limits for Deep Learning</a>.&rdquo; arXiv preprint arXiv:2402.17457 (2024).</p>
</li>
</ul>
<h4 id=b-theory-and-practice-for-transformers>[B] Theory and Practice for Transformers</h4>
<ul>
<li>
<p>[][B-1] Cowsik, Aditya, et al. &ldquo;<a href=https://arxiv.org/pdf/2403.02579v1.pdf>Geometric Dynamics of Signal Propagation Predict Trainability of Transformers</a>.&rdquo; arXiv preprint arXiv:2403.02579 (2024).</p>
</li>
<li>
<p>[][B-2] Noci, Lorenzo, et al. &ldquo;<a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/ae0cba715b60c4052359b3d52a2cff7f-Paper-Conference.pdf>Signal propagation in transformers: Theoretical perspectives and the role of rank collapse</a>.&rdquo; Advances in Neural Information Processing Systems 35 (2022): 27198-27211.</p>
</li>
<li>
<p>[][B-3] Malladi, Sadhika, et al. &ldquo;<a href=https://proceedings.mlr.press/v202/malladi23a/malladi23a.pdf>A kernel-based view of language model fine-tuning</a>.&rdquo; International Conference on Machine Learning. PMLR, 2023.</p>
</li>
<li>
<p>[][B-4] Hayou, Soufiane, Nikhil Ghosh, and Bin Yu. &ldquo;<a href=https://arxiv.org/pdf/2402.12354.pdf>LoRA+: Efficient Low Rank Adaptation of Large Models</a>.&rdquo; arXiv preprint arXiv:2402.12354 (2024). Together with the original <a href=https://arxiv.org/pdf/2106.09685>LoRA paper</a></p>
</li>
</ul>
<h4 id=c-others>[C] Others</h4>
<ul>
<li>
<p>[][C-1] Bordelon, Blake, Alexander Atanasov, and Cengiz Pehlevan. &ldquo;<a href=https://arxiv.org/pdf/2402.01092v1.pdf>A Dynamical Model of Neural Scaling Laws</a>.&rdquo; arXiv preprint arXiv:2402.01092 (2024).</p>
</li>
<li>
<p>[][C-2] Bahri, Yasaman et al. &ldquo;<a href=https://arxiv.org/pdf/2102.06701.pdf>Explaining Neural Scaling Laws</a>.&rdquo; 2021.</p>
</li>
<li>
<p>[][C-2] Kumar, Tanishq, et al. &ldquo;<a href=https://arxiv.org/pdf/2310.06110.pdf>Grokking as the transition from lazy to rich training dynamics</a>.&rdquo; arXiv preprint arXiv:2310.06110 (2023).</p>
</li>
<li>
<p>[][C-4] Papyan, Vardan, X. Y. Han, and David L. Donoho. &ldquo;<a href=https://www.pnas.org/doi/full/10.1073/pnas.2015509117>Prevalence of neural collapse during the terminal phase of deep learning training</a>.&rdquo; Proceedings of the National Academy of Sciences 117.40 (2020): 24652-24663.</p>
</li>
<li>
<p>[][C-5] Adityanarayanan Radhakrishnan et al. &ldquo;<a href=https://arxiv.org/pdf/2212.13881.pdf>Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features</a>.&rdquo; 2023.</p>
</li>
</ul>
<h3 id=more-information>More information</h3>
<h3 id=contact-and-thanks>Contact and Thanks</h3>
<ul>
<li><a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a>, EIC, Huazhong University of Science and Technology</li>
</ul>
<p>The organizers are grateful for support from NSFC via fund NSFC-62206101.</p>
</main>
<footer>
<script defer src=//zhenyu-liao.github.io/js/math-code.js></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script defer src=//zhenyu-liao.github.io/js/center-img.js></script>
<hr>
Â© <a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a> 2016 &ndash; 2023 | <a href="https://scholar.google.fr/citations?user=SPYhJV8AAAAJ&hl=en">Google Scholar</a> | <a href=https://www.researchgate.net/profile/Zhenyu_Liao>Research Gate</a> | <a href=https://orcid.org/0000-0002-1915-8559>ORCID</a> | <a href=https://github.com/Zhenyu-LIAO/>Github</a> | <a href=https://zhuanlan.zhihu.com/RandomMatrixTheory>RMT on Zhihu</a>
</footer>
</body>
</html>