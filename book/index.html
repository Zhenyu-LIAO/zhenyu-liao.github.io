<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Book | Zhenyu Liao's Page</title>
<link rel=stylesheet href=/css/style.css>
<link rel=stylesheet href=/css/fonts.css>
</head>
<body>
<nav>
<ul class=menu>
<li><a href=/>Home</a></li>
<li><a href=/publications/>Publications</a></li>
<li><a href=/activities/>Activities</a></li>
<li><a href=/book/>Book</a></li>
<li><a href=/teaching/>Teaching</a></li>
<li><a href=/index.xml>RSS subscribe</a></li>
</ul>
<hr>
</nav>
<div class=article-meta>
<h1><span class=title>Book</span></h1>
</div>
<main>
<img src=/img/RMT4ML_1stpage.png style=max-width:10%;min-width:50px;float:left>
<p><strong>Random Matrix Methods for Machine Learning</strong></p>
<p>By Romain Couillet and <strong>Zhenyu Liao</strong> (alphabetical order)</p>
<p>Book draft is available <a href=/pdf/RMT4ML.pdf>here</a> and exercise solutions <a href=/pdf/RMT4ML_solution.pdf>here</a>.</p>
<hr>
<h2 id=abstract>Abstract</h2>
<p>Numerous and large dimensional data is now a default setting in modern machine learning (ML). Standard ML algorithms, starting with kernel methods such as support vector machines and graph-based methods like the PageRank algorithm, were however initially designed out of small dimensional intuitions and tend to misbehave, if not completely collapse, when dealing with real-world large datasets. Random matrix theory has recently developed a broad spectrum of tools to help understand this new curse of dimensionality, to help repair or completely recreate the suboptimal algorithms, and most importantly to provide new intuitions to deal with modern data mining.
This monograph primarily aims to deliver these intuitions, by providing a digest of the recent theoretical and applied breakthroughs of random matrix theory into machine learning. Targeting a broad audience, spanning from undergraduate students interested in statistical learning to AI engineers and researchers alike, the mathematical prerequisites to the book are minimal (basics of probability theory, linear algebra and real and complex analysis are sufficient): as opposed to introductory books in the mathematical literature of random matrix theory and large dimensional statistics, the theoretical focus here is restricted to the essential requirements to machine learning applications. These applications range from detection, statistical inference and estimation, to graph- and kernel-based supervised, semi-supervised and unsupervised classification, as well as neural networks: for these, a precise theoretical prediction of the algorithm performance (often inaccessible when not resorting to a random matrix analysis), large dimensional insights, methods of improvement, along with a fundamental justification of the wide-scope applicability of the methods to real data, are provided.
Most methods, algorithms, and figure proposed in the monograph are coded in <code>MATLAB</code> and <code>Python</code> and made available to the readers (<a href=https://github.com/Zhenyu-LIAO/RMT4ML>https://github.com/Zhenyu-LIAO/RMT4ML</a>). The monograph also contains a series of exercises of two types: short exercises with corrections appended to the end of the book to familiarize the reader with the basic theoretical notions and tools in random matrix analysis, as well as long guided exercises to apply these tools to further concrete machine learning applications.</p>
</main>
<footer>
<script src=//zhenyu-liao.github.io/js/math-code.js></script>
<script async src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script async src=//zhenyu-liao.github.io/js/center-img.js></script>
<hr>
Â© <a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a> 2016 &ndash; 2021 | <a href="https://scholar.google.fr/citations?user=SPYhJV8AAAAJ&hl=en">Google Scholar</a> | <a href=https://www.researchgate.net/profile/Zhenyu_Liao>Research Gate</a> | <a href=https://orcid.org/0000-0002-1915-8559>ORCID</a> | <a href=https://github.com/Zhenyu-LIAO/>Github</a> | <a href=https://zhuanlan.zhihu.com/RandomMatrixTheory>RMT on Zhihu</a>
</footer>
</body>
</html>