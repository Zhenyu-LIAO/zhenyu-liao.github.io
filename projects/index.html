<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Projects | Zhenyu Liao's Page</title>
<link rel=stylesheet href=/css/style.css>
<link rel=stylesheet href=/css/fonts.css>
</head>
<body>
<nav>
<ul class=menu>
<li><a href=/>Home</a></li>
<li><a href=/publications/>Publications</a></li>
<li><a href=/projects/>Projects</a></li>
<li><a href=/book/>Book</a></li>
<li><a href=/collaborators/>Collaborators</a></li>
<li><a href=/teaching/>Teaching</a></li>
<li><a href=/links/>Links</a></li>
<li><a href=/post_page/>Posts</a></li>
<li><a href=/index.xml>RSS subscribe</a></li>
</ul>
<hr>
</nav>
<div class=article-meta>
<h1><span class=title>Projects</span></h1>
</div>
<main>
<h4 id=nsfc-62206101-fundamental-limits-of-pruning-deep-neural-network-models-via-random-matrix-methods>NSFC-62206101: Fundamental Limits of Pruning Deep Neural Network Models via Random Matrix Methods</h4>
<p>This project (2023.01-2025.12) is led by myself, and focuses on the fundamental theoretical limits of pruning as well as quantization of deep neural networks. The objective of this project is to propose, by developing the mathematical tools of random matrix theory, high-dimensional statistics, and optimization theory, a quantitative theory to characterize the &ldquo;performance and complexity tradeoff&rdquo; in modern deep neural nets.
This project leads to the following scientific publications:</p>
<ol>
<li>
<p>Z. Feng, Y. Wang, J. Li, F. Yang, J. Lou, T. Mi, R. C. Qiu, <strong>Z. Liao</strong>, &ldquo;<a href=https://ieeexplore.ieee.org/document/10772332>Robust and Communication-Efficient Federated Domain Adaptation via Random Features</a>&rdquo;, <em>IEEE Transactions on Knowledge and Data Engineering</em>, 2024.</p>
</li>
<li>
<p>J. Wei, X. Lee, <strong>Z. Liao</strong>, T. Palpanas, B. Peng &ldquo;<a href>Subspace Collision: An Efficient and Accurate Framework for High-dimensional Approximate Nearest Neighbor Search</a>&rdquo;, <em>SIGMOD International Conference on Management of Data (<strong>SIGMOD 2025</strong>)</em>, 2025. <a href=https://arxiv.org/abs/2411.14754>preprint</a></p>
</li>
<li>
<p>W. Yang, Z. Wang, X. Mai, Z. Ling, R. C. Qiu, <strong>Z. Liao</strong> &ldquo;<a href=https://ieeexplore.ieee.org/document/10715081/>Inconsistency of ESPRIT DoA Estimation for Large Arrays and a Correction via RMT</a>&rdquo; (<font color=red>Best Student Paper Candidate</font>), <em>IEEE 32nd European Signal Processing Conference (<strong>EUSIPCO 2024</strong>)</em>, 2024.</p>
</li>
<li>
<p>Z. Ling, L. Li, Z. Feng, Y. Zhang, F. Zhou, R. C. Qiu, <strong>Z. Liao</strong> &ldquo;<a href=https://proceedings.mlr.press/v235/ling24a.html>Deep Equilibrium Models are Almost Equivalent to Not-so-deep Explicit Models for High-dimensional Gaussian Mixtures</a>&rdquo;, <em>The Forty-first International Conference on Machine Learning (<strong>ICML 2024</strong>)</em>, 2024. <a href=https://arxiv.org/abs/2402.02697>preprint</a></p>
</li>
<li>
<p>Y. Song, K. Wan, <strong>Z. Liao</strong>, H. Xu, G. Caire, S. Shamai, &ldquo;<a href=https://ieeexplore.ieee.org/document/10619077>An Achievable and Analytic Solution to Information Bottleneck for Gaussian Mixtures</a>&rdquo;, <em>2024 IEEE International Symposium on Information Theory (<strong>ISIT 2024</strong>)</em>, 2024.</p>
</li>
<li>
<p>Y. Wang, Z. Feng, <strong>Z. Liao</strong>, &ldquo;<a href=https://ieeexplore.ieee.org/document/10626024>FedRF-Adapt: Robust and Communication-Efficient Federated Domain Adaptation via Random Features</a>&rdquo;, <em><a href=https://workshop-tpmln2024.webflow.io/>Workshop on Timely and Private Machine Learning over Networks</a></em>, <em>2024 IEEE International Conference on Acoustics, Speech and Signal Processing (<strong>ICASSPW 2024</strong>)</em>, 2024.</p>
</li>
<li>
<p>J. Wang, S. Zhang, J. Cai, <strong>Z. Liao</strong>, C. Arenz, R. Betzholz, &ldquo;<a href=https://journals.aps.org/pra/abstract/10.1103/PhysRevA.108.022408>Robustness of random-control quantum-state tomography</a>&rdquo;, Physical Review A 108 (2 Aug. 2023), 022408.</p>
</li>
<li>
<p>Y. Chitour, <strong>Z. Liao</strong>, R. Couillet, &ldquo;<a href=https://www.aimsciences.org/article/doi/10.3934/mcrf.2022021>A geometric approach of gradient descent algorithms in linear neural networks</a>&rdquo;, <em>Mathematical Control and Related Fields</em>, 13(3) (2023), 918–945.</p>
</li>
<li>
<p>L. Gu, Y. Du, Y. Zhang, D. Xie, S. Pu, R. C. Qiu, <strong>Z. Liao</strong>, &ldquo;<a href=https://proceedings.neurips.cc/paper_files/paper/2022/hash/185087ea328b4f03ea8fd0c8aa96f747-Abstract-Conference.html>&ldquo;Lossless&rdquo; Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach</a>&rdquo;, <em>The 36th Conference on Neural Information Processing Systems (<strong>NeurIPS'2022</strong>)</em>, 2022.</p>
</li>
</ol>
<hr>
<h4 id=ccf-hikvision-open-fund-20210008-random-matrix-theory-and-information-bottleneck-for-neural-network-compression>CCF-Hikvision Open Fund 20210008: Random Matrix Theory and Information Bottleneck for Neural Network Compression</h4>
<p>This project is led by Prof. <a href="https://scholar.google.com/citations?user=EyzJHVsAAAAJ&hl=zh-CN">Kai Wan</a> and myself as PI, and investigates efficient compression schemes of large-scale neural network models with strong theoretical guarantees. The project leads to the following scientific publications:</p>
<ol>
<li>
<p>H. Tiomoko, <strong>Z. Liao</strong>, R. Couillet, &ldquo;<a href="https://openreview.net/forum?id=qwULHx9zld">Random matrices in service of ML footprint: ternary random features with no performance loss</a>&rdquo;, <em>The Tenth International Conference on Learning Representations (<strong>ICLR'2022</strong>)</em>, 2022. <a href=https://arxiv.org/abs/2110.01899>preprint</a></p>
</li>
<li>
<p>L. Gu, Y. Du, Y. Zhang, D. Xie, S. Pu, R. C. Qiu, <strong>Z. Liao</strong>, &ldquo;<a href=https://proceedings.neurips.cc/paper_files/paper/2022/hash/185087ea328b4f03ea8fd0c8aa96f747-Abstract-Conference.html>&ldquo;Lossless&rdquo; Compression of Deep Neural Networks: A High-dimensional Neural Tangent Kernel Approach</a>&rdquo;, <em>The 36th Conference on Neural Information Processing Systems (<strong>NeurIPS'2022</strong>)</em>, 2022.</p>
</li>
<li>
<p>Y. Song, K. Wan, <strong>Z. Liao</strong>, G. Caire, &ldquo;<a href=https://arxiv.org/abs/2302.03549>An Achievable and Analytic Solution to Information Bottleneck for Gaussian Mixtures</a>&rdquo;, 2023.</p>
</li>
</ol>
<p>See more details of the project in Chinese <a href=https://www.ccf.org.cn/Collaboration/Enterprise_Fund/News/2021-10-22/746058.shtml>here</a>.</p>
<hr>
<h4 id=nsfc-12141107-mathematical-theory-and-methods-for-reconfigurable-intelligent-surface-ris-assisted-wireless-communication>NSFC-12141107: Mathematical theory and methods for Reconfigurable Intelligent Surface (RIS) assisted wireless communication</h4>
<p>This project (2022.01-2025.12) is led by <a href="https://scholar.google.com/citations?user=FTLNXX8AAAAJ&hl=zh-CN">Prof. R. C. Qiu</a> and investigates the (information) theoretical limits of RIS assisted wireless communication system, dynamical system, as well as the theory of large dimensional random matrices.</p>
<p>See the project homepage <a href=http://eic.hust.edu.cn/prj/nsfc-qcm/>here</a>.</p>
</main>
<footer>
<script defer src=//zhenyu-liao.github.io/js/math-code.js></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script defer src=//zhenyu-liao.github.io/js/center-img.js></script>
<hr>
© <a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a> 2016 &ndash; 2024 | <a href="https://scholar.google.fr/citations?user=SPYhJV8AAAAJ&hl=en">Google Scholar</a> | <a href=https://www.researchgate.net/profile/Zhenyu_Liao>Research Gate</a> | <a href=https://orcid.org/0000-0002-1915-8559>ORCID</a> | <a href=https://github.com/Zhenyu-LIAO/>Github</a> | <a href=https://zhuanlan.zhihu.com/RandomMatrixTheory>RMT on Zhihu</a>
</footer>
</body>
</html>