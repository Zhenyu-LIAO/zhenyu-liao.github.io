<!doctype html><html lang=en-us>
<head>
<meta name=generator content="Hugo 0.91.2">
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>Home | Zhenyu Liao's Page</title>
<link rel=stylesheet href=/css/style.css>
<link rel=stylesheet href=/css/fonts.css>
</head>
<body>
<nav>
<ul class=menu>
<li><a href=/>Home</a></li>
<li><a href=/publications/>Publications</a></li>
<li><a href=/projects/>Projects</a></li>
<li><a href=/book/>Book</a></li>
<li><a href=/teaching/>Teaching</a></li>
<li><a href=/links/>Links</a></li>
<li><a href=/post_page/>Posts</a></li>
<li><a href=/index.xml>RSS subscribe</a></li>
</ul>
<hr>
</nav>
<h1 id=about-me>About Me</h1>
<p>I am now an Associate Professor in the <a href=http://ei.hust.edu.cn/English/Home.htm>School of Electronic Information and Communications (EIC)</a>, <a href=http://english.hust.edu.cn/>Huazhong University of Science & Technology (HUST)</a>, China.
Prior to that, I was a Postdoctoral Fellow at the <a href=https://statistics.berkeley.edu/>Department of Statistics</a> and at the <a href=https://www.icsi.berkeley.edu/icsi/>International Computer Science Institute (ICSI)</a> of University of California, Berkeley, hosted by <a href=https://www.stat.berkeley.edu/~mmahoney/>Michael Mahoney</a>.
I obtained my Ph.D. from <a href=http://www.centralesupelec.fr/en>CentraleSupélec</a>, <a href=https://www.universite-paris-saclay.fr/en>University Paris-Saclay</a>, France, in 2019, where I worked with <a href=https://polaris.imag.fr/romain.couillet/index2.html>Romain Couillet</a> and <a href=https://l2s.centralesupelec.fr/en/u/chitour-yacine/>Yacine Chitour</a>.</p>
<p>My work primarily focuses on the statistical and computational aspects of machine learning, signal processing, and data science. Currently, I am particularly interested in high-dimensional statistics and <a href=https://en.wikipedia.org/wiki/Random_matrix>random matrix theory</a>, as well as their interactions with (deep or not so deep) neural networks.</p>
<p>For more information, see <a href=img/homepage.JPG>here</a> for my headshot and <a href=/bio_CN.txt>here</a> for a short bio in Chinese.</p>
<hr>
<h2 id=curriculum-vitae>Curriculum Vitae</h2>
<p>Here is my CV in <a href=/pdf/liao_CV.pdf>English</a> and in <a href=/pdf/Chines_CV.pdf>Chinese</a>.</p>
<hr>
<h3 id=rmta-seminar>RMTA seminar</h3>
<p>A monthly online seminar on Random Matrix Theory and Applications on behalf of the Chinese Association of Random Matrix Theory.</p>
<p>The seminar aims to cover both the theoretical and applied aspects of Random Matrices.</p>
<p>Please visit the <a href=https://rmta-seminar.github.io>seminar page</a> of more information, and feel free to follow the <a href=mailto:rmta-seminar+subscribe@googlegroups.com>RMTA seminar Google group</a>!</p>
<hr>
<h2 id=news>News</h2>
<h3 id=2026>2026</h3>
<ul>
<li>[Jun 2026] I will be talking at the 2nd &ldquo;<a href=https://ivado.ca/en/events/2nd-workshop-uncertainty-in-ai/>Uncertainty in AI</a>&rdquo; workshop at <a href=https://ivado.ca/en/>IVADO</a>, as a part of IVADO&rsquo;s Thematic Semester &ldquo;<a href=https://ivado.ca/en/thematic-programs/statistical-foundations-of-ai/>Statistical Foundations of AI</a>&rdquo;, in Montreal from June 8th to June 11th 2026.</li>
<li>[Jun 2026] I am co-organizing (together with <a href=https://www.columbia.edu/~rd2714/>Rishabh Dudeja</a>, <a href=https://lsec.cc.ac.cn/~mjj/>Junjie Ma</a>, and <a href=https://sites.google.com/site/malekiarian/>Arian Maleki</a>) the &ldquo;<a href=https://hds-workshop-isit.github.io/index.html>Universality and Dynamics in High-Dimensional Learning and Inference</a>&rdquo; Workshop at <a href=https://2026.ieee-isit.org/>2026 IEEE International Symposium on Information Theory (ISIT 2026)</a>, June 28 to July 3 in Guangzhou, China. <font color=red>Consider submitting your work!</font> See <a href=/pdf/ISIT2026workshop-CfP-final.pdf>CfP</a>.</li>
<li>[May 2026] Two papers at <a href=https://2026.ieeeicassp.org/>2026 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP 2026)</a>! Check <a href=/publications>here</a> for more information. See you in Barcelona!</li>
<li>[March 2026] Prof. <a href=https://l2s.centralesupelec.fr/en/u/chitour-yacine/>Yacine CHITOUR</a> will deliver a &ldquo;<a href=/pdf/Crash_Course_by_Yacine_CHITOUR.pdf>crash course on machine learning theory</a>&rdquo; from <strong>March 09</strong> to <strong>March 13</strong>, 2026, at HUST, Wuhan, China.</li>
</ul>
<h3 id=2025-and-earlier>2025 and Earlier</h3>
<ul>
<li>[Dec 2025] I am very grateful to be supported by the <font color=red>National Key R&D Program of China (Youth Scientist Program)</font> on my research. The project focuses on the Theoretical Foundations of Large-scale Machine Learning Models.
The project is led by <a href=https://gr.xjtu.edu.cn/en/web/junshu>Jun Shu</a> at Xi&rsquo;an Jiaotong University and jointly conducted with <a href=https://hongxin001.github.io/>Hongxin Wei</a> and <a href=https://www.sustech.edu.cn/en/faculties/liceng.html>Zeng Li</a> at SUSTech, as well as <a href="https://scholar.google.com/citations?user=BabePTkAAAAJ&hl=en">Zenan Ling</a> at HUST.</li>
<li>[Nov 2025] I will be serving as an <strong>Area Chair</strong> for <a href=https://icml.cc/>ICML</a> 2026.</li>
<li>[Sep 2025] I&rsquo;m very grateful to be supported by the <font color=red>National Natural Science Foundation of China (General Program)</font> on my research.
This project is jointly conducted with Prof. <a href=https://sds.cuhk.edu.cn/en/teacher/947>Cosme Louart</a> at SDS, CUHK-SZ.</li>
<li>[Sep 2025] I will be serving as an <strong>Area Chair</strong> for <a href=https://virtual.aistats.org/>AISTATS</a> 2026.</li>
<li>[Aug 2025] I will be serving as an <strong>Area Chair</strong> for <a href=https://iclr.cc/>ICLR</a> 2026.</li>
<li>[May 2025] Our work on the statistical inversion bias of random sampling, with application to sub-sampled Newton methods, has been selected as an <font color=red>Oral</font> paper at <strong>ICML 2025</strong>!
In this paper, we show that commonly used random sampling techniques ranging from uniform and leverage, to Hadamard transform-based, all exhibit statistical inversion bias.
And, this bias can be effectively corrected (sometimes <strong>without</strong> additional computational burden) using RMT.
This work is led by my PhD student <strong><a href="https://scholar.google.com/citations?user=RV5kIXXNbzoC">Chengmei Niu</a></strong>&mdash;check out our preprint <a href=https://arxiv.org/abs/2502.13583>here</a>!</li>
<li>Check out the <a href=/pdf/RMT_4_Modern_ML.pdf>work-in-progress lecture notes</a> on &ldquo;Random Matrix Theory for Modern Machine Learning.&rdquo;
Feedback and suggestions are very welcome&mdash;feel free to share your thoughts!</li>
<li>[May 2024] One paper at <strong>ICML'2024</strong> on RMT for Deep Equilibrium Model (DEQ, a typical implicit NN model) that provides very <strong>explicit</strong> connections between implicit and explicit NNs. Given a DEQ, check our preprint <a href=https://arxiv.org/abs/2402.02697>here</a> for the recipe to design an &ldquo;equivalent&rdquo; simple explicit NN!</li>
<li>[Aug 2022] One paper at <strong>NeurIPS'2022</strong> on the eigenspectral structure of Neural Tangent Kernel (NTK) of fully-connected <em>Deep</em> neural networks for Gaussian mixture input data, with a compelling application to &ldquo;lossless&rdquo; sparsification and quantization of DNN models! This extends our <a href="https://openreview.net/forum?id=qwULHx9zld">previous paper</a> at <strong>ICLR'2022</strong>. See more details <a href="https://openreview.net/forum?id=NaW6T93F34m">here</a>.</li>
<li>[Jun 2022] Our book &ldquo;<font color=red>Random Matrix Methods for Machine Learning</font>&rdquo; with Cambridge University Press, see more details <a href=https://www.cambridge.org/core/books/random-matrix-methods-for-machine-learning/6B681EB69E58B5F888EDB689C160C682>here</a>!</li>
</ul>
<hr>
<h3 id=openings>Openings</h3>
<p>I am always looking for self-motivated interns, masters, and Ph.D. students, with a strong background and genuine interest in statistics and machine learning theory and methods on (but not limited to) the following topics:
theoretical foundations and methods of deep learning, model compression, and efficient computation (e.g., randomized numerical linear algebra, stochastic and/or distributed optimization).</p>
<p>If you are passionate about these areas and eager to work in a collaborative and research-driven environment, feel free to reach out.</p>
<hr>
<h2 id=contact-me>Contact Me</h2>
<p><strong>E-mail</strong>: zhenyu_liao <strong>at</strong> hust.edu.cn</p>
<blockquote>
</blockquote>
<footer>
<script defer src=//zhenyu-liao.github.io/js/math-code.js></script>
<script defer src="//mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script defer src=//zhenyu-liao.github.io/js/center-img.js></script>
<hr>
© <a href=https://zhenyu-liao.github.io/>Zhenyu Liao</a> 2016 &ndash; 2026 | <a href="https://scholar.google.fr/citations?user=SPYhJV8AAAAJ&hl=en">Google Scholar</a> | <a href=https://www.researchgate.net/profile/Zhenyu_Liao>Research Gate</a> | <a href=https://orcid.org/0000-0002-1915-8559>ORCID</a> | <a href=https://github.com/Zhenyu-LIAO/>Github</a> | <a href=https://rmta-seminar.github.io/>RMTA Seminar</a>
</footer>
</body>
</html>